# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

# RL-Games配置文件 - LeapHand连续旋转任务
# 基于官方manipulation任务配置，针对连续旋转任务优化

params:
  seed: 42  # 随机种子，保证实验可重复性

  # 环境包装器裁剪参数
  env:
    clip_observations: 5.0  # 观测值裁剪范围，防止数值过大
    clip_actions: 1.0      # 动作值裁剪范围，限制动作幅度

  # 算法相关配置
  algo:
    name: a2c_continuous  # 使用连续动作空间的A2C算法

  # 模型配置
  model:
    name: continuous_a2c_logstd  # 使用带有log标准差的连续A2C模型

  # 神经网络结构配置
  network:
    name: actor_critic     # 使用Actor-Critic架构
    separate: True        # 启用非对称Actor-Critic，Actor和Critic使用独立的网络
    
    # 连续动作空间配置
    space:
      continuous:
        mu_activation: None         # 均值输出层不使用激活函数
        sigma_activation: None      # 标准差输出层不使用激活函数

        mu_init:
          name: default            # 使用默认初始化方法
        sigma_init:
          name: const_initializer  # 标准差使用常数初始化
          val: 0                  # 初始化值为0
        fixed_sigma: True         # 使用固定的标准差
    
    # MLP网络配置
    mlp:
      units: [512, 512, 256]  # 三层隐藏层，神经元数量分别为512,512,256
      activation: elu         # 使用ELU激活函数
      d2rl: False            # 不使用D2RL架构
      
      initializer:
        name: default        # 使用默认权重初始化方法
      regularizer:
        name: None          # 不使用正则化

    rnn:  # 循环神经网络配置
      name: gru  # 使用GRU单元
      units: 256  # GRU隐藏单元数
      layers: 1  # GRU层数
      before_mlp: true  # RNN是否在MLP之前
      concat_input: true  # 是否连接输入
      layer_norm: true  # 是否使用层归一化

  # 模型加载配置
  load_checkpoint: False  # 是否加载检查点
  load_path: ''          # 检查点加载路径

  # 训练配置
  config:
    name: leaphand_object_rot  # 任务名称
    env_name: rlgpu               # 使用GPU训练环境
    device: 'cuda:0'              # 使用第一块GPU
    device_name: 'cuda:0'         # 设备名称
    multi_gpu: False              # 不使用多GPU训练
    ppo: True                     # 使用PPO算法
    mixed_precision: False        # 不使用混合精度训练
    normalize_input: True         # 对输入进行归一化
    normalize_value: True         # 对价值进行归一化
    value_bootstrap: True         # 使用价值引导
    num_actors: -1               # 并行环境数量，由脚本自动配置
    reward_shaper:
      scale_value: 1.0           # 奖励缩放因子
    normalize_advantage: True     # 对优势函数进行归一化
    gamma: 0.99                  # 折扣因子
    tau: 0.95                    # GAE-Lambda参数
    learning_rate: 5e-4          # 学习率
    lr_schedule: adaptive        # 自适应学习率调整
    schedule_type: standard      
    kl_threshold: 0.01           # KL散度阈值，控制策略更新幅度
    score_to_win: 100000          # 获胜分数阈值
    max_epochs: 10000             # 最大训练轮数（采集数据->更新模型的完整流程）
    save_best_after: 200         # 100轮后开始保存最佳模型
    save_frequency: 200          # 每50轮保存一次
    print_stats: True            # 打印训练统计信息
    grad_norm: 1.0               # 梯度裁剪范数
    entropy_coef: 0.002          # 熵正则化系数，用于鼓励探索
    truncate_grads: True         # 启用梯度裁剪
    e_clip: 0.2                  # PPO裁剪范围
    horizon_length: 32         # 每horizon_length个step进行一次策略更新
    minibatch_size: 16384           # 增大批量以适应更长的horizon（先前是800）
    mini_epochs: 5               # 每次更新的迭代次数（采集数据的循环利用次数）
    critic_coef: 4               # Critic损失权重
    clip_value: True             # 是否裁剪价值估计
    clip_actions: False          # 是否在训练时裁剪动作
    seq_length: 4               # 序列长度
    bounds_loss_coef: 0.0001     # 动作边界损失系数

    player:  # 测试配置
      deterministic: True  # 是否使用确定性策略
      games_num: 100000  # 测试游戏次数
      print_stats: True  # 是否打印统计信息

# Population-Based Training (PBT) 配置
# 使用方法：启动多个进程，每个进程设置不同的policy_idx
# 例如：agent.pbt.enabled=True agent.pbt.policy_idx=0
pbt:
  enabled: False                   # 是否启用PBT（默认关闭，通过命令行开启）
  policy_idx: 0                    # 当前策略索引 [0, num_policies-1]
  num_policies: 6                  # 种群大小（推荐6-8个）
  directory: "."                   # 共享工作目录（存放所有策略的checkpoint）
  workspace: "pbt_workspace"       # PBT工件子目录名
  # objective格式说明：rl_games会在extras["episode"]的键前加"episode."前缀
  # RewardsCfg中的属性名 → Episode_Reward/<name> → episode.Episode_Reward/<name>
  # CurriculumCfg中的属性名 → Curriculum/<name> → episode.Curriculum/<name>
  objective: "episode.Episode_Reward/track_orientation_inv_l2"  # 使用方向跟踪奖励作为评分指标
  interval_steps: 2000000          # PBT迭代间隔（环境步数），约每2M步评估一次
  threshold_std: 0.1               # 标准差阈值（用于确定领导者/落后者）
  threshold_abs: 0.05              # 绝对阈值
  mutation_rate: 0.25              # 每个参数的变异概率
  change_range: [1.1, 2.0]         # 变异因子范围 [min, max]
  mutation:                        # 可变异的超参数列表
    agent.params.config.learning_rate: "mutate_float"   # 学习率
    agent.params.config.entropy_coef: "mutate_float"    # 熵正则化系数
    agent.params.config.grad_norm: "mutate_float"       # 梯度裁剪阈值
    agent.params.config.gamma: "mutate_discount"        # 折扣因子
    agent.params.config.tau: "mutate_discount"          # GAE参数